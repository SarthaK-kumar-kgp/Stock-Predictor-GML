# -*- coding: utf-8 -*-
"""GML_mine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_5Xas0VWflwTm-ukQFeTjHkzaROGMVIS

'data.csv' and run all
"""

!git clone https://github.com/google-research/timesfm.git
!cd timesfm
!pip install -e .[torch]

import torch
import pandas as pd
from timesfm.src.timesfm.timesfm_2p5.timesfm_2p5_torch import TimesFM_2p5_200M_torch
from timesfm.src.timesfm.configs import ForecastConfig
import gc
from tqdm import tqdm

torch.set_float32_matmul_precision('high')

device= 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

class EmbeddingGraph():

    def __init__(self, dataset):

        self.dataset= dataset
        self.tickers= list(set(dataset['ticker'].tolist()))
        self.stock_price_df= dataset[['Date', 'ticker', 'Close']]

        self.embeddings= torch.zeros((len(self.tickers), 32, 1280), dtype= torch.float16, device= device)
        self.i= None
        self.loss= torch.nn.CosineSimilarity()


    def get_embeddings_(self):
        # for each ticker,
        #   for each feature,
        #     get feature embeddings
        # Final output:- Ticker, 1 embedding per feature

        model = TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch")
        model.compile(
          ForecastConfig(
              max_context=1024,
              max_horizon=256,
              normalize_inputs=False,
              use_continuous_quantile_head=True,
              force_flip_invariance=False,
              infer_is_positive=True,
              fix_quantile_crossing=True,
          )
        )
        model.model.to(device)
        model.model.eval()

        batch_size= 16
        max_context= 1024
        batch_tensor= torch.zeros((batch_size, max_context), dtype= torch.float32)
        batch_mask= torch.zeros((batch_size, max_context), dtype= torch.bool)

        i= 0
        print('Getting Embeddings')
        for ticker in tqdm(self.tickers):
            i+=1

            ticker_df= self.stock_price_df[self.stock_price_df['ticker']==ticker].drop(columns= ['ticker'])
            ticker_df['Date']= pd.to_datetime(ticker_df['Date']).dt.date
            ticker_df= ticker_df.sort_values(by='Date', ascending= True).reset_index(drop= True)

            if (w:=len(ticker_df['Close']))>max_context:
                context= ticker_df['Close'].tolist()[-max_context:]
                mask= [False]*max_context
            else:
                context= [0]*(max_context-w) + ticker_df['Close'].tolist()
                mask= [True]*(max_context-w) + [False]*w


            #context= 1+torch.tensor(context, dtype= torch.float32, device= device)
            context= torch.log(1+torch.tensor([0]+context, dtype= torch.float32, device= device))
            context= (context[torch.arange(1, context.numel())]-context[torch.arange(0,context.numel()-1)])
            model_inputs= context.unsqueeze(dim= 0).view(1, 32, 32)
            model_mask= torch.tensor(mask, dtype= torch.bool, device= device).unsqueeze(dim= 0).view(1, 32, 32)

            with torch.no_grad():
              out= model.model.forward(inputs= model_inputs, masks= model_mask)
              self.embeddings[i-1, ...]+= out[0][1][0, :, :]

            # batch_tensor[(i-1)%batch_size, :]= torch.tensor(context, dtype= torch.float16)
            # batch_mask[(i-1)%batch_size, :]= torch.tensor(mask, dtype= torch.bool)


            # if i%batch_size==0 or i==len(self.tickers):
            #     # process batch

            #     model_inputs= batch_tensor.view(batch_size, 32, 32).to(device)
            #     model_masks= batch_mask.view(batch_size, 32, 32).to(device)

            #     with torch.no_grad():
            #       out= model.model.forward(inputs= model_inputs, masks= model_masks)

            #     k=0
            #     for j in range(((i-1)//batch_size)*batch_size +1, min(i, len(self.tickers))):
            #         self.embeddings[j, :, :]+= out[0][2][k, :, :]
            #         k+=1

        del model
        gc.collect()

    def get_adjacency_(self):

        self.adjacency= torch.zeros((len(self.tickers), len(self.tickers), 32), dtype= torch.float16, device= device)

        print('Getting Adjacency')
        for i, ticker in tqdm(enumerate(self.tickers)):
            for j, ticker in enumerate(self.tickers):

                if i==j:
                    self.adjacency[i, j, :]= 1
                else:
                    self.adjacency[i, j, :]= (self.loss(self.embeddings[i, :, :].squeeze(), self.embeddings[j, :, :].squeeze()))


        self.adjacency= self.adjacency[:, :, 31].squeeze()
        self.i= i
        self.adjacency.to('cpu')
        torch.save(self.adjacency, 'embeddings_adjacency.pt')

df.read_csv('embed')

df = pd.read_csv('stock_data_collated.csv')

graph= EmbeddingGraph(df)

df.head()

"""# New section"""

out= graph.get_embeddings_()

graph.get_adjacency_()

tensor= torch.load('embeddings_adjacency.pt')

from matplotlib import pyplot as plt

plt.spy((tensor==1).cpu(), markersize=1)

# after computing self.adjacency
import pandas as pd

# Convert tensor to CPU and numpy
adj_np = self.adjacency.cpu().numpy()

# Wrap it in a DataFrame with tickers as both index and columns
adj_df = pd.DataFrame(adj_np, index=self.tickers, columns=self.tickers)

# Save to CSV (optional)
adj_df.to_csv('embeddings_adjacency.csv')

# Or keep in memory
self.adjacency_df = adj_df

df2= pd.DataFrame(tensor.cpu().numpy())
df2.to_csv('embeddings_continous.csv')

df2= pd.DataFrame((tensor>=0.925).int().cpu().numpy())
df2.to_csv('embeddings_binary.csv')

df2

class EmbeddingGraph():

    def __init__(self, dataset):
        self.dataset = dataset
        self.tickers = list(set(dataset['ticker'].tolist()))
        self.stock_price_df = dataset[['Date', 'ticker', 'Close']]
        self.embeddings = torch.zeros((len(self.tickers), 32, 1280),
                                      dtype=torch.float16, device=device)
        self.i = None
        self.loss = torch.nn.CosineSimilarity()
        self.adjacency_df = None  # will store labeled adjacency

    def get_embeddings_(self):
        model = TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch")
        model.compile(
            ForecastConfig(
                max_context=1024,
                max_horizon=256,
                normalize_inputs=False,
                use_continuous_quantile_head=True,
                force_flip_invariance=False,
                infer_is_positive=True,
                fix_quantile_crossing=True,
            )
        )
        model.model.to(device)
        model.model.eval()

        max_context = 1024

        print('Getting Embeddings')
        for i, ticker in enumerate(tqdm(self.tickers)):
            ticker_df = self.stock_price_df[self.stock_price_df['ticker']==ticker].drop(columns=['ticker'])
            ticker_df['Date'] = pd.to_datetime(ticker_df['Date']).dt.date
            ticker_df = ticker_df.sort_values(by='Date', ascending=True).reset_index(drop=True)

            w = len(ticker_df['Close'])
            if w > max_context:
                context = ticker_df['Close'].tolist()[-max_context:]
                mask = [False] * max_context
            else:
                context = [0]*(max_context-w) + ticker_df['Close'].tolist()
                mask = [True]*(max_context-w) + [False]*w

            context = torch.log(1 + torch.tensor([0]+context, dtype=torch.float32, device=device))
            context = context[1:] - context[:-1]  # differences

            model_inputs = context.unsqueeze(0).view(1, 32, 32)
            model_mask = torch.tensor(mask, dtype=torch.bool, device=device).unsqueeze(0).view(1, 32, 32)

            with torch.no_grad():
                out = model.model.forward(inputs=model_inputs, masks=model_mask)
                self.embeddings[i, ...] += out[0][1][0, :, :]

        del model
        gc.collect()

    def get_adjacency_(self):
        import pandas as pd

        n = len(self.tickers)
        self.adjacency = torch.zeros((n, n, 32), dtype=torch.float16, device=device)

        print('Getting Adjacency')
        for i, ticker_i in enumerate(tqdm(self.tickers)):
            for j, ticker_j in enumerate(self.tickers):
                if i == j:
                    self.adjacency[i, j, :] = 1
                else:
                    self.adjacency[i, j, :] = self.loss(
                        self.embeddings[i, :, :].squeeze(),
                        self.embeddings[j, :, :].squeeze()
                    )

        # Use last embedding slice for adjacency
        adj_np = self.adjacency[:, :, -1].cpu().numpy()

        # Convert to DataFrame with tickers as labels
        self.adjacency_df = pd.DataFrame(adj_np, index=self.tickers, columns=self.tickers)

        # Save CSV
        self.adjacency_df.to_csv('embeddings_adjacency.csv')
        print('Adjacency matrix saved to embeddings_adjacency.csv')

# Assuming you already have your dataset loaded as a DataFrame `df`
# and the device is defined (e.g., device = 'cuda' or 'cpu')

# 1. Initialize the graph
graph = EmbeddingGraph(df)

# 2. Compute embeddings for each ticker
graph.get_embeddings_()

# 3. Compute the adjacency matrix
graph.get_adjacency_()

# 4. Access the adjacency matrix as a DataFrame with ticker labels
adjacency_df = graph.adjacency_df

# Example: check the first 5 rows
adjacency_df.head()

similarity = adjacency_df.loc['AAPL', 'MSFT']
similarity

# adjacency_df is already a DataFrame with tickers as index and columns
adjacency_df = graph.adjacency_df.copy()

# 1️⃣ Save raw similarity values
adjacency_df.to_csv('embeddings_continuous.csv')

# 2️⃣ Save binary adjacency (threshold = 0.8)
threshold = 0.8
adjacency_binary_df = (adjacency_df >= 0.95).astype(int)
adjacency_binary_df.to_csv('embeddings_binary.csv')

adjacency_binary_df

adjacency_df

